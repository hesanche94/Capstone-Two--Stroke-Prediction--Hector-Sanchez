{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b1cff05-af97-48f1-8fca-6c1bb45a8eb6",
   "metadata": {},
   "source": [
    "### ***PREDICTING STROKE RISK USING PATIENT HEALTH DATA - MODELING***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffea588-9cf2-4672-a44c-ce19d2c5aa26",
   "metadata": {},
   "source": [
    "**Moedling Plan**\n",
    "\n",
    "1. Select and Fit Models (Two to Three Models)\n",
    "\n",
    "    Given that we're working on stroke prediction (a classification problem), a few models to consider include:\n",
    "        Logistic Regression (baseline model)\n",
    "        Random Forest Classifier\n",
    "        Gradient Boosting\n",
    "\n",
    "    Code Outline: For each model, we will:\n",
    "        Fit the model to our training dataset.\n",
    "        Evaluate its performance using metrics like accuracy, precision, recall, and F1 score.\n",
    "        Optionally perform hyperparameter tuning (e.g., using GridSearchCV or cross-validation).\n",
    "   \n",
    "3. Hyperparameter Tuning\n",
    "\n",
    "    Use GridSearchCV or RandomizedSearchCV to optimize parameters of at least one model\n",
    "\n",
    "3. Compare Model Performance\n",
    "\n",
    "    After fitting our models and tuning hyperparameters, compare them based on:\n",
    "        Accuracy\n",
    "        Precision\n",
    "        Recall\n",
    "        F1 Score\n",
    "        Confusion Matrix (for a more detailed breakdown)\n",
    "    Create a comparison table of these metrics for each model.\n",
    "\n",
    "4. Select the Best Model\n",
    "\n",
    "    Based on the performance metrics, determine which model best fits our criteria.\n",
    "    Discuss why this model is the best for this project (consider not only accuracy but also scalability, computational efficiency, and interpretability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5d054f30-7bed-460c-82a8-0f515e98d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we'll start by importing the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4c3dd251-3e45-4114-ab81-7c83b41531b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we'll load the scaled/preprocessed datasets that we created from the previous steps of this project\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/hecsa/Springboard/Springboard Github/Springboard/Data Science Capstone Two/dataset/stroke_data_scaled.csv'\n",
    "stroke_data_scaled = pd.read_csv(file_path)\n",
    "\n",
    "X_train = pd.read_csv('C:/Users/hecsa/Springboard/Springboard Github/Springboard/Data Science Capstone Two/dataset/X_train.csv')\n",
    "X_test = pd.read_csv('C:/Users/hecsa/Springboard/Springboard Github/Springboard/Data Science Capstone Two/dataset/X_test.csv')\n",
    "y_train = pd.read_csv('C:/Users/hecsa/Springboard/Springboard Github/Springboard/Data Science Capstone Two/dataset/y_train.csv')\n",
    "y_test = pd.read_csv('C:/Users/hecsa/Springboard/Springboard Github/Springboard/Data Science Capstone Two/dataset/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b1fead63-cef3-4312-9c1b-733ecdba6575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                       int64\n",
      "age                                    float64\n",
      "hypertension                             int64\n",
      "heart_disease                            int64\n",
      "avg_glucose_level                      float64\n",
      "bmi                                    float64\n",
      "age_group                               object\n",
      "age_hypertension_interaction           float64\n",
      "gender_Male_True                          bool\n",
      "gender_Other_True                         bool\n",
      "ever_married_Yes_True                     bool\n",
      "work_type_Never_worked_True               bool\n",
      "work_type_Private_True                    bool\n",
      "work_type_Self-employed_True              bool\n",
      "work_type_children_True                   bool\n",
      "Residence_type_Urban_True                 bool\n",
      "smoking_status_formerly smoked_True       bool\n",
      "smoking_status_never smoked_True          bool\n",
      "smoking_status_smokes_True                bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8cd3398d-c4e6-4cec-a0f6-f46d9b900adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding to categorical columns\n",
    "X_train_encoded = pd.get_dummies(X_train, drop_first=True)  # One-hot encoding\n",
    "X_test_encoded = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "# Align columns in case one-hot encoding created different columns\n",
    "X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='left', axis=1, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3be8bd0d-419e-476f-bb6f-7626723d25e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stroke\n",
      "0         3901\n",
      "1          187\n",
      "Name: count, dtype: int64\n",
      "stroke\n",
      "0         960\n",
      "1          62\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check class imbalance: Use value_counts() to see if you have many more non-stroke cases than stroke cases.\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "78661081-4141-4663-9d23-f70022f805a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store evaluation metrics for each model\n",
    "evaluation_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "689c7eca-0b41-4a58-a355-a87c83eb26ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model\n",
    "log_reg = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "log_reg.fit(X_train_encoded, y_train)\n",
    "y_pred_log = log_reg.predict(X_test_encoded)\n",
    "\n",
    "# Evaluation Metrics for Logistic Regression\n",
    "log_reg_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_log),\n",
    "    'Precision': precision_score(y_test, y_pred_log),\n",
    "    'Recall': recall_score(y_test, y_pred_log),\n",
    "    'F1 Score': f1_score(y_test, y_pred_log)\n",
    "}\n",
    "evaluation_metrics['Logistic Regression'] = log_reg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cac8255e-121a-473d-8b40-32a10879624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "rf_clf.fit(X_train_encoded, y_train)\n",
    "y_pred_rf = rf_clf.predict(X_test_encoded)\n",
    "\n",
    "# Evaluation Metrics for Random Forest\n",
    "rf_clf_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_rf),\n",
    "    'Precision': precision_score(y_test, y_pred_rf),\n",
    "    'Recall': recall_score(y_test, y_pred_rf),\n",
    "    'F1 Score': f1_score(y_test, y_pred_rf)\n",
    "}\n",
    "evaluation_metrics['Random Forest'] = rf_clf_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "725778b1-bd39-4648-920c-d547f8834759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "gb_clf.fit(X_train_encoded, y_train)\n",
    "y_pred_gb = gb_clf.predict(X_test_encoded)\n",
    "\n",
    "# Evaluation Metrics for Gradient Boosting\n",
    "gb_clf_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_gb),\n",
    "    'Precision': precision_score(y_test, y_pred_gb),\n",
    "    'Recall': recall_score(y_test, y_pred_gb),\n",
    "    'F1 Score': f1_score(y_test, y_pred_gb)\n",
    "}\n",
    "evaluation_metrics['Gradient Boosting'] = gb_clf_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a87f7886-94a6-4263-85b0-d816ba9ae0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Model\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate the class imbalance ratio\n",
    "# This ratio is the number of negative examples (class 0) divided by the number of positive examples (class 1)\n",
    "imbalance_ratio = y_train.value_counts()[0] / y_train.value_counts()[1]\n",
    "\n",
    "# Initialize the XGBoost model with scale_pos_weight to address class imbalance\n",
    "gb_clf = XGBClassifier(random_state=42, scale_pos_weight=imbalance_ratio)\n",
    "\n",
    "# Fit the model on the training data\n",
    "gb_clf.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_gb = gb_clf.predict(X_test_encoded)\n",
    "\n",
    "# Calculate the evaluation metrics for XGBoost\n",
    "gb_clf_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_gb),\n",
    "    'Precision': precision_score(y_test, y_pred_gb, zero_division=0),\n",
    "    'Recall': recall_score(y_test, y_pred_gb, zero_division=0),\n",
    "    'F1 Score': f1_score(y_test, y_pred_gb, zero_division=0)\n",
    "}\n",
    "\n",
    "# Adding XGBoost metrics to the evaluation dictionary\n",
    "evaluation_metrics['XGBoost'] = gb_clf_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "229c1404-9ba1-4f86-b083-11d495b870f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Model Performance:\n",
      "Accuracy: 0.6947\n",
      "Precision: 0.1469\n",
      "Recall: 0.8387\n",
      "F1 Score: 0.2500\n",
      "\n",
      "Random Forest Model Performance:\n",
      "Accuracy: 0.9393\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Gradient Boosting Model Performance:\n",
      "Accuracy: 0.9384\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "XGBoost Model Performance:\n",
      "Accuracy: 0.9168\n",
      "Precision: 0.2195\n",
      "Recall: 0.1452\n",
      "F1 Score: 0.1748\n"
     ]
    }
   ],
   "source": [
    "# Display the evaluation metrics for all models\n",
    "for model_name, metrics in evaluation_metrics.items():\n",
    "    print(f\"\\n{model_name} Model Performance:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"{metric_name}: {metric_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3a893b51-0f86-4e44-aacb-ebcfb3580333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 324 candidates, totalling 972 fits\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],        # Number of trees\n",
    "    'learning_rate': [0.01, 0.1, 0.2],      # Step size shrinkage\n",
    "    'max_depth': [3, 5, 7],                 # Maximum depth of trees\n",
    "    'subsample': [0.8, 1.0],                # Fraction of samples used for training each tree\n",
    "    'colsample_bytree': [0.8, 1.0],         # Fraction of features used for each tree\n",
    "    'gamma': [0, 1, 5],                     # Minimum loss reduction for a split to happen\n",
    "    'scale_pos_weight': [imbalance_ratio]    # Handling class imbalance\n",
    "}\n",
    "\n",
    "# Initialize XGBoost model\n",
    "xgb_clf = XGBClassifier(random_state=42)\n",
    "\n",
    "# Set up GridSearchCV for XGBoost\n",
    "grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, cv=3, scoring='f1', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit GridSearchCV on the training data\n",
    "grid_search.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Tuned XGBoost Model\n",
    "best_xgb = grid_search.best_estimator_\n",
    "y_pred_best_xgb = best_xgb.predict(X_test_encoded)\n",
    "\n",
    "# Evaluation Metrics for Tuned XGBoost\n",
    "tuned_xgb_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_best_xgb),\n",
    "    'Precision': precision_score(y_test, y_pred_best_xgb),\n",
    "    'Recall': recall_score(y_test, y_pred_best_xgb),\n",
    "    'F1 Score': f1_score(y_test, y_pred_best_xgb)\n",
    "}\n",
    "\n",
    "# Adding the tuned XGBoost metrics to the evaluation dictionary\n",
    "evaluation_metrics['Tuned XGBoost'] = tuned_xgb_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "500c2974-7990-4e51-96d9-456e460694c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Model Performance:\n",
      "Accuracy: 0.6947\n",
      "Precision: 0.1469\n",
      "Recall: 0.8387\n",
      "F1 Score: 0.2500\n",
      "\n",
      "Random Forest Model Performance:\n",
      "Accuracy: 0.9393\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Gradient Boosting Model Performance:\n",
      "Accuracy: 0.9384\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "XGBoost Model Performance:\n",
      "Accuracy: 0.9168\n",
      "Precision: 0.2195\n",
      "Recall: 0.1452\n",
      "F1 Score: 0.1748\n",
      "\n",
      "Tuned XGBoost Model Performance:\n",
      "Accuracy: 0.8004\n",
      "Precision: 0.1858\n",
      "Recall: 0.6774\n",
      "F1 Score: 0.2917\n"
     ]
    }
   ],
   "source": [
    "# Display the evaluation metrics for all models\n",
    "for model_name, metrics in evaluation_metrics.items():\n",
    "    print(f\"\\n{model_name} Model Performance:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"{metric_name}: {metric_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "24a6d163-0765-4b40-98ca-838bbea270da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy  Precision    Recall  F1 Score\n",
      "0  Logistic Regression  0.694716   0.146893  0.838710  0.250000\n",
      "1        Random Forest  0.939335   0.000000  0.000000  0.000000\n",
      "2    Gradient Boosting  0.938356   0.000000  0.000000  0.000000\n",
      "3              XGBoost  0.916830   0.219512  0.145161  0.174757\n",
      "4        Tuned XGBoost  0.800391   0.185841  0.677419  0.291667\n"
     ]
    }
   ],
   "source": [
    "# Create a comparison table for all models\n",
    "models = list(evaluation_metrics.keys())\n",
    "accuracy = [metrics['Accuracy'] for metrics in evaluation_metrics.values()]\n",
    "precision = [metrics['Precision'] for metrics in evaluation_metrics.values()]\n",
    "recall = [metrics['Recall'] for metrics in evaluation_metrics.values()]\n",
    "f1_scores = [metrics['F1 Score'] for metrics in evaluation_metrics.values()]\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': models,\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1_scores\n",
    "})\n",
    "\n",
    "# Display the comparison DataFrame\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1592c5a-db98-45b5-b26a-636e9cec1dc8",
   "metadata": {},
   "source": [
    "**Evaluating the Metrics for Stroke Prediction:**\n",
    "\n",
    "Accuracy: \n",
    "    This is the percentage of correct predictions. While higher accuracy is generally good, accuracy can be misleading in imbalanced datasets, such as in \n",
    "    stroke prediction (where fewer positive cases of strokes exist compared to negative cases).\n",
    "    Random Forest and Gradient Boosting have very high accuracy (~93-94%), but their precision, recall, and F1 score are extremely low, meaning theyâ€™re \n",
    "    not identifying stroke cases well at all.\n",
    "\n",
    "Precision: \n",
    "    This metric indicates how many predicted positive cases (stroke) are actually true positives. In healthcare, precision is important to avoid too many \n",
    "    false positives, which could lead to unnecessary interventions.\n",
    "    XGBoost and Tuned XGBoost show better precision compared to other models. Tuned XGBoost has a slightly lower precision than XGBoost but still performs \n",
    "    better than Logistic Regression.\n",
    "\n",
    "Recall: \n",
    "    This is critical in healthcare. Recall (sensitivity) measures the ability to correctly identify all actual stroke cases. Missing a stroke case (false \n",
    "    negative) could have severe consequences, so high recall is essential.\n",
    "    Tuned XGBoost stands out here with 0.677 recall, significantly higher than all other models. Logistic Regression also has good recall, but other \n",
    "    metrics (like precision and F1 score) are weaker.\n",
    "\n",
    "F1 Score: \n",
    "    The F1 score is a balance between precision and recall. It is important for imbalanced datasets because it balances identifying true positives \n",
    "    (recall) with avoiding false positives (precision). In this case:\n",
    "    Tuned XGBoost has the highest F1 score at 0.2917, indicating the best trade-off between precision and recall, which is important for stroke prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79b58fb-8290-4924-8a45-c4b1b8c78129",
   "metadata": {},
   "source": [
    "**Conclusion and Best Model Choice:**\n",
    "\n",
    "Tuned XGBoost is the best choice for this stroke prediction task based on overall performance, especially its recall (0.677) and F1 score (0.2917). \n",
    "\n",
    "High recall ensures that the model identifies the majority of stroke cases, which is vital in healthcare to avoid missing high-risk patients.\n",
    "\n",
    "Logistic Regression also has high recall but performs poorly in precision, which means it's likely to generate many false positives (patients incorrectly predicted to have a stroke).\n",
    "    \n",
    "XGBoost (untuned) performs worse than the tuned version, especially in recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23313b6c-0f5f-413d-a5ab-27a67f925eb6",
   "metadata": {},
   "source": [
    "**Why Tuned XGBoost is the Best Fit:**\n",
    "\n",
    "Recall is the highest, which is critical in healthcare contexts (missing fewer stroke cases).\n",
    "\n",
    "F1 Score balances precision and recall, making it a reliable choice for dealing with imbalanced data.\n",
    "\n",
    "Scalability: XGBoost is designed to handle large datasets and is efficient in terms of computational performance.\n",
    "\n",
    "Interpretability: While not as interpretable as Logistic Regression, XGBoost offers feature importance insights, which can help explain what factors contribute most to stroke prediction.\n",
    "\n",
    "Tuning improved the performance, so the model is optimized for this dataset.\n",
    "\n",
    "Thus, moving forward with Tuned XGBoost is the most reasonable choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4caa5176-d13d-412a-beba-6986d9b48178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned XGBoost model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Saving the Tuned XGBoost Model:\n",
    "import joblib\n",
    "\n",
    "# Save the tuned XGBoost model to a file\n",
    "joblib.dump(best_xgb, 'tuned_xgboost_model.pkl')\n",
    "\n",
    "print(\"Tuned XGBoost model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef6b856-5ea6-4d0c-a2f8-58a723aaa342",
   "metadata": {},
   "source": [
    "**Follow the instructions below to load the saved model in the future**\n",
    "\n",
    "#Load the saved XGBoost model from the file\n",
    "\n",
    "loaded_xgb_model = joblib.load('tuned_xgboost_model.pkl')\n",
    "\n",
    "print(\"Tuned XGBoost model loaded successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
